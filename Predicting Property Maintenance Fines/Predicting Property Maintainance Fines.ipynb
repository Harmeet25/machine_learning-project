{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Property Maintenance Fines (or Blight)\n",
    "\n",
    "This is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "The Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. [Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time.\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). Nonetheless, we encourage you to look into data from other Detroit datasets to help inform feature creation and model selection. We recommend taking a look at the following related datasets:\n",
    "\n",
    "* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf)\n",
    "* [Trades Permits](https://data.detroitmi.gov/Property-Parcels/Trades-Permits/635b-dsgv)\n",
    "* [Improve Detroit: Submitted Issues](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn)\n",
    "* [DPD: Citizen Complaints](https://data.detroitmi.gov/Public-Safety/DPD-Citizen-Complaints-2016/kahe-efs3)\n",
    "* [Parcel Map](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf)\n",
    "\n",
    "___\n",
    "\n",
    "We provide you with two data files for use in training and validating your models: train.csv and test.csv. Each row in these two files corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. Compliance, as well as a handful of other variables that will not be available at test-time, are only included in train.csv.\n",
    "\n",
    "Note: All tickets where the violators were found not responsible are not considered during evaluation. They are included in the training set as an additional source of data for visualization, and to enable unsupervised and semi-supervised approaches. However, they are not included in the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**File descriptions** (Use only this data for training your model!)\n",
    "\n",
    "    train.csv - the training set (all tickets issued 2004-2011)\n",
    "    test.csv - the test set (all tickets issued 2012-2016)\n",
    "    addresses.csv & latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Your predictions will be given as the probability that the corresponding blight ticket will be paid on time.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC). \n",
    "___\n",
    "\n",
    "For this assignment, create a function that trains a model to predict blight ticket compliance in Detroit using `train.csv`. Using this model, return a series of length 61001 with the data being the probability that each corresponding ticket from `test.csv` will be paid, and the index being the ticket_id.\n",
    "\n",
    "Example:\n",
    "\n",
    "    ticket_id\n",
    "       284932    0.531842\n",
    "       285362    0.401958\n",
    "       285361    0.105928\n",
    "       285338    0.018572\n",
    "                 ...\n",
    "       376499    0.208567\n",
    "       376500    0.818759\n",
    "       369851    0.018528\n",
    "       Name: compliance, dtype: float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## A Quick Summary\n",
    "Using Python, the data was imported and then processed using several techniques. For instance:\n",
    "\n",
    "    Specific features were dropped due to either data leakage or inconsistencies\n",
    "    Violation address was converted to the respected lat/lon coordinate pairs\n",
    "    Categorical data was 'one-hot-encoded' with a defined frequency threshold\n",
    "    Feature engineering was performed on two datetime features by taking the day difference\n",
    "    Models were trained with the roc_auc scoring, as opposed to just accuracy (allow for skewedness in data!)\n",
    "    Two classifier algorithms were fitted, Gradient Boosting and Logistic Regression\n",
    "    'Some' fine tuning was applied to increase the auc score\n",
    "\n",
    "Gradient Boosting performed the best, with a 'fine-tuned' score on a 5-fold CV of 0.8057 (rounded). To summarize:\n",
    "\n",
    "    Gradient Boosting Roc_Auc Score = 0.805666\n",
    "    Logistic Regression Roc_Auc Score = 0.787156\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GB algorithm\n",
    "from sklearn.linear_model import LogisticRegression # LR algorithm\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV # Additional scklearn functions\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc # Scoring metrics to be used\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the libraries, we can read in the data. Note that the dataframes for address.csv and latlons.csv are merged on the ticket_id with the train dataframe. We will use the lat/lon data as a replacement for the violation street name and street number (note that the violation zip code was dropped from the set as the majority is NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'hearing_date, ticket_issued_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/harmeetpurohit/Documents/ML project/Predicting Property Maintenance Fines/Predicting Property Maintainance Fines.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dtypes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mticket_issued_date\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhearing_date\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m} \u001b[39m# set known date labels to strings for conversion to dt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m parse_dates \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mticket_issued_date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhearing_date\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# create list of date labels\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mtrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,encoding \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mISO-8859-1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         dtype \u001b[39m=\u001b[39;49m dtypes, parse_dates \u001b[39m=\u001b[39;49m parse_dates) \u001b[39m# Read in train.csv\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m'\u001b[39m,encoding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mISO-8859-1\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                        dtype \u001b[39m=\u001b[39m dtypes, parse_dates \u001b[39m=\u001b[39m parse_dates) \u001b[39m# Read in test.csv\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Predicting%20Property%20Maintenance%20Fines/Predicting%20Property%20Maintainance%20Fines.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Let's import addresses and accompanying lat/lons and merge on address\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1680\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_usecols_names(\n\u001b[1;32m    156\u001b[0m             usecols,\n\u001b[1;32m    157\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames,  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_parse_dates_presence(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnames)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_noconvert_columns()\n\u001b[1;32m    164\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py:230\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m    220\u001b[0m missing_cols \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    221\u001b[0m     \u001b[39msorted\u001b[39m(\n\u001b[1;32m    222\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m missing_cols:\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing column provided to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mparse_dates\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmissing_cols\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[39m# Convert positions to actual column names\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    235\u001b[0m     col \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(col, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m col \u001b[39min\u001b[39;00m columns) \u001b[39melse\u001b[39;00m columns[col]\n\u001b[1;32m    236\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols_needed\n\u001b[1;32m    237\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'hearing_date, ticket_issued_date'"
     ]
    }
   ],
   "source": [
    "## Read in data\n",
    "dtypes = {'ticket_issued_date': 'str', 'hearing_date': 'str'} # set known date labels to strings for conversion to dt\n",
    "parse_dates = ['ticket_issued_date', 'hearing_date'] # create list of date labels\n",
    "\n",
    "df_train = pd.read_csv('train.csv',encoding = \"ISO-8859-1\", \n",
    "                        dtype = dtypes, parse_dates = parse_dates) # Read in train.csv\n",
    "df_test = pd.read_csv('test.csv',encoding = \"ISO-8859-1\", \n",
    "                       dtype = dtypes, parse_dates = parse_dates) # Read in test.csv\n",
    "\n",
    "# Let's import addresses and accompanying lat/lons and merge on address\n",
    "df_address = pd.read_csv('addresses.csv', encoding = \"ISO-8859-1\") # Read in addresses.csv (locations of violations in Detroit)\n",
    "df_latlons = pd.read_csv('latlons.csv', encoding = \"ISO-8859-1\") # Lat/lons of violation locactions\n",
    "df_id_latlons = pd.merge(df_address, df_latlons, how='inner', on='address') # Merge the address and lat/lons on ticket_id\n",
    "\n",
    "# Drop address label now that it's merged\n",
    "df_id_latlons.drop('address', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop obvious labels\n",
    "Any labels that appear to cause either data leakage, inconsistency, or have a majoirty of NaN's are removed from the dataset. Furthermore, we will also drop sampels corresponding to a target value of NaN (blight offenders that were found not responsible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drop data labels that should not be used in analysis\n",
    "#1 Get labels that are inconsistent with test\n",
    "inconsistent_labels = ['payment_date', 'payment_status', 'collection_status', \n",
    "               'compliance_detail', 'balance_due', 'payment_amount']\n",
    "\n",
    "#2 Get data leakage labels\n",
    "data_leak_labels = ['violator_name', 'inspector_name']\n",
    "\n",
    "#3 Get NaN's labels (col's with majority of NaN's)\n",
    "NaN_labels = (df_train.isnull().sum() / len(df_train)) <= 0.50\n",
    "maj_NaN_labels = NaN_labels[NaN_labels==False].index.tolist()\n",
    "\n",
    "#4 Combine labels and drop from train\n",
    "labels_to_remove = []\n",
    "labels_to_remove.extend(inconsistent_labels + data_leak_labels + maj_NaN_labels)\n",
    "df_train.drop(labels_to_remove, axis=1, \n",
    "        inplace = True)\n",
    "\n",
    "#5 Remove NaN's from compliance label (just keep targets)\n",
    "compliance_to_keep = df_train.compliance.notnull()\n",
    "df_train = df_train.loc[compliance_to_keep, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing steps 1-5 above, the labels that remain in the training set are as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before steps 1-5 were taken, we had:\n",
    "\n",
    "    ticket_id\n",
    "    agency_name\n",
    "    inspector_name\n",
    "    violator_name\n",
    "    violation_street_number, violation_street_name, violation_zip_code\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country\n",
    "    ticket_issued_date \n",
    "    hearing_date \n",
    "    violation_code, violation_description \n",
    "    disposition \n",
    "    fine_amount\n",
    "    admin_fee \n",
    "    state_fee \n",
    "    late_fee \n",
    "    discount_amount\n",
    "    clean_up_cost\n",
    "    judgment_amount\n",
    "    grafitti_status\n",
    "    payment_amount \n",
    "    payment_date\n",
    "    payment_status\n",
    "    balance_due\n",
    "    collection_status\n",
    "    compliance_detail\n",
    " \n",
    "Moving forward, we now have only 22 labels, as opposed to the original 33 (excluding compliance, our target label). Note that the removal fo the 11 labels was due to the preprocessing of the data in steps 1-5 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "To begin, we will remove any remaining labels that we do not want to include in the analysis. We chose the mailing location of the violator to not be included:\n",
    "    \n",
    "    ['mailing_address_str_number', 'mailing_address_str_name', \n",
    "                  'city', 'state', 'zip_code', 'country']\n",
    "\n",
    "It is possible that this data can indeed have a positive impact of the AUC train score, but it is assumed that this data can vary too greatly in the test set (and future test sets), and thus our model will not be generalized well enough. However, it is worth considering and should be included in the model when feature engineering becomes a must.\n",
    "\n",
    "Secondly, we will replace the violation street name / number with the corresponding lat/lon and fill the lat/lon NaN's with the most frequent value.\n",
    "\n",
    "Third, we'll splice out the columns that are purely objects, and call our own Categories function to convert said columns into categories, specify a frequency threshold, and bucket categories below the threshold as **'unknown'**.\n",
    "\n",
    "Finally, we'll call get_dummies to complete our one-hot-encoding process of the object data. Note that each label was carefully examined to ensure that one-hot-encoding was the best approach to categorize the data. We noted that each object label were strings that were *not sorted*. If any object was indeed a sorted label (e.g. (low, medium, high), then we would have simply called the method cat.codes and not the function get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Process data\n",
    "#1 Drop remaining address columns\n",
    "labels_address = ['mailing_address_str_number', 'mailing_address_str_name', \n",
    "                  'city', 'state', 'zip_code', 'country']\n",
    "df_train.drop(labels_address, axis = 1, \n",
    "        inplace = True)\n",
    "\n",
    "#2 Let's merge violation street name / number with corresponding lat/lons\n",
    "# and fill na's with most frequent of each column\n",
    "df_id_latlons = df_id_latlons.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "df_train = pd.merge(df_train, df_id_latlons, how='inner', on='ticket_id')\n",
    "df_train.drop(['violation_street_number', 'violation_street_name'], axis = 1, \n",
    "              inplace = True)\n",
    "\n",
    "#3 Get labels of objects for one-hot encoding\n",
    "col_obj = df_train.dtypes[df_train.dtypes == 'object'].index.tolist()\n",
    "\n",
    "# Convert object data into categories (fill less than threshold with 'unknown' cat)\n",
    "def Categories(series):\n",
    "    threshold = 100 # frequency of categories\n",
    "    unknown_cat = '<unknown>' # name of additional 'unknown' category\n",
    "    for series in series:\n",
    "        count = df_train[series].value_counts()\n",
    "        categories_to_keep = count[count > threshold].index.tolist()\n",
    "        df_train[series] = pd.Categorical(df_train[series], \n",
    "                categories = categories_to_keep, ordered=True)\n",
    "        df_train[series] = df_train[series].cat.add_categories(unknown_cat).fillna(unknown_cat)\n",
    "\n",
    "Categories(col_obj) # Update dataframe with categorical data\n",
    "\n",
    "#4 Call get_dummies\n",
    "df_train = pd.get_dummies(df_train, columns = col_obj) # One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting here that the threshold of categorical frequency was taken 100. This was a design choice and has obvious consequences on the test score. Simply reducing the threshold to allow for categories that have less than 100 appearances will lead to basically empty feature sets when get_dummies is called. Therefore, this will only add computational time and resources, all for a score that won't see much change. \n",
    "\n",
    "However, one must take note on the number of categories present in the feature. If the majority are equal or approximate to the threshold, then the threshold should either not be used at all, or altered. *The idea* is to take the majority of the categories that are in the feature, and bin the remaining categories into one 'unknown' category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Next, we will engineer a new feature by taking the difference between ticket_issued_date and the hearing_date. We update the new label with the difference in days. Furthermore, we perform a last check on any columns that contain NaN, and fill those NaN's with the respected mean value of the column.\n",
    "\n",
    "Note that we find the columns that contain NaN's to speed up the fillna() process (as opposed to calculating the mean for every column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering\n",
    "# Remove the two date labels, and engineer a new feature.\n",
    "# This feature will simply be the time between the hearing data and the \n",
    "# issue ticket date.\n",
    "col_date = ['hearing_date', 'ticket_issued_date']\n",
    "df_train['hearing_issued_date_diff'] = (df_train[col_date[0]]\n",
    "        - df_train[col_date[1]]).dt.days\n",
    "df_train.drop(col_date, axis=1, \n",
    "        inplace = True)\n",
    "\n",
    "NaN_in_labels = df_train.columns[df_train.isnull().any()].tolist()\n",
    "df_train.fillna(df_train[NaN_in_labels].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show that there are zero NaN values in our set, and that we are ready to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training the model\n",
    "We can define a function that performs the fit, caluclates the accuracy and roc_auc score of the training data alone, and then performs cross-validation on the training set to return the averaged roc_auc score of the left-out data. \n",
    "\n",
    "It's a good idea to make note of the accuracy and roc_auc score of the entire training set. \n",
    "\n",
    "**Accuracy Score on training set**: This metric is helpful in evaluating just how good the model can predict the label of the postitive class. Of course, this metric alone is not a good indicator of the performance of the model, as this metric assumes that any probability above 0.5 should be labeled the positive class. This alone neglects the possibility for skewedness in the target values, but is useful in evauluating variance/bias. \n",
    "\n",
    "**ROC_AUC Score on training set**: Looking at the roc_auc score, in conjunction with accuracy, can help define how well the model is performing. The 'auc score' basically takes into account the skewedness of the data by evaulating over many thresholds (not just 0.5) when classifying the data. The area under the roc curve (roc_auc) is essentually a number that explains how much, over a range of thresholds, does the true positives reflect the false positives. A higher roc_auc means that more of the threshold range corresponds to a high true positive rate. \n",
    "\n",
    "** ROC_AUC Score on cross-validation set**: Looking at just the training data is again not a good practice, as one does not know if there is a bias/variance problem of the model. One must analyze the hold-out set from the CV results to ensure that the model is generalizing well enough. \n",
    "\n",
    "Of course, using the CV score isn't enough either, as we'll have to ensure that unseen, 'new' data still generalizes well to the model (i.e. test.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelCV(alg, dtrain, predictors, performCV=True, cv_folds=5):      \n",
    "    # Predict training set\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, dtrain[predictors], \n",
    "                                                    dtrain[target], \n",
    "                                                    cv=cv_folds, \n",
    "                                                    scoring='roc_auc')\n",
    "    \n",
    "    # Print model report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, \n",
    "          dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], \n",
    "          dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"AUC_CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" \n",
    "              % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),\n",
    "                 np.max(cv_score)))\n",
    "        \n",
    "def plotFeatureImp(alg, predictors):\n",
    "    #Plot feature importance\n",
    "    feature = 10\n",
    "    feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)[:feature]\n",
    "    ax = feat_imp.plot(kind = 'barh', title=\"Feature Importance [Top 10]\")\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.box(on=None)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plotROCAUC(alg, dtrain):\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(dtrain[target], dtrain_predprob)\n",
    "    roc_auc = roc_auc_score(dtrain[target], dtrain_predprob)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlim([-0.01, 1.00])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.plot(fpr, tpr, lw=3, label='ROC curve (area = {:0.2f})'.format(roc_auc))\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title('ROC curve (Binary Classifier)', fontsize=16)\n",
    "    plt.legend(loc='lower right', fontsize=13)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "    plt.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify our id and target column, get our features that we'd like to include, identify our classifier, fit, then call the modelCV function to give us a modest 'model report'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols \n",
    "target = 'compliance'\n",
    "IDcol = 'ticket_id'\n",
    "predictors = [x for x in df_train.columns if x not in [target, IDcol]]\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "gbm0.fit(df_train[predictors], df_train[target])\n",
    "\n",
    "modelCV(gbm0, df_train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot feature importance\n",
    "plt.style.use('seaborn-pastel')\n",
    "plotFeatureImp(gbm0, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above score for a default Gradient Boosting Classifier was about 0.8023 (cv mean test score). We can also note the 10 most important features that contributed to the predictions made by the classifier in Figure 1. We can futher optimize and tune the model by performing a grid search over a range of params (GridSearchCV). We perform a grid search and vary the 'n_estimators' parameter for GBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators': np.arange(20, 41, 10).tolist()}\n",
    "gsearch1 = GridSearchCV(GradientBoostingClassifier(learning_rate=0.05,min_samples_split=500,\n",
    "                                                   min_samples_leaf=50,max_depth=8,\n",
    "                                                   max_features='sqrt',subsample=0.8,random_state=10), \n",
    "                        param_grid = param_test1, scoring='roc_auc', iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(df_train[predictors],df_train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the gridsearch object and print the best parameter (in this case, just n_estimator) and the highest mean roc_auc score on the cv test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note here. The n_estimators parameter reached the extreme (20) on the range chosen (20, 30, 40). We should increase the lower end of the param range to see if the parameter lies below the current extreme. Also, the test score increased from 0.8023 to 0.8057. Let's continue to optimize by first extending the range for n_estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators': np.arange(10, 31, 10).tolist()}\n",
    "gsearch1 = GridSearchCV(GradientBoostingClassifier(learning_rate=0.05,min_samples_split=500,\n",
    "                                                   min_samples_leaf=50,max_depth=8,\n",
    "                                                   max_features='sqrt',subsample=0.8,random_state=10), \n",
    "                        param_grid = param_test1, scoring='roc_auc', iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(df_train[predictors],df_train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lowering the range does not help, as the best value for n_estimators is set at 20. Let's perform another grid search over the follow:\n",
    "\n",
    "    max_depth [5-15 at int of 2]\n",
    "    min_samples_split [50-200 at int of 50]\n",
    "    \n",
    "We'll make sure to update n_estimators to 20 from the previous gridsearch (gsearch1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth': np.arange(5,11,1).tolist(), 'min_samples_split': np.arange(100,1501,100).tolist()}\n",
    "\n",
    "gsearch2 = GridSearchCV(GradientBoostingClassifier(learning_rate=0.05, n_estimators=20, \n",
    "                                                   max_features='sqrt', subsample=0.8, \n",
    "                                                   random_state=10), \n",
    "                        param_grid = param_test2, scoring='roc_auc', iid=False, cv=5)\n",
    "\n",
    "gsearch2.fit(df_train[predictors],df_train[target])\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the auc score generated here is lower than the previous. It however can be assumed that gridsearching over other parameters besides 'max_depth' and 'min_samples_split' will lead to a higher score. Increasing the number of trees ('n_estimators') and lowering the 'learning_rate' can lead to significant gains in score. \n",
    "\n",
    "Once the fine tuning is complete, the results can be analyed against the unseen test set. The test set will have to be processed in the same manner as the training set. One must be careful not to remove samples from the test set when dealing with NaN's, as samples of a test set should be treated as real world samples that hold significance. Of course, we're expecing the test score to be slightly below the finalized cv score. \n",
    "\n",
    "Let's run our 'final' model of gradient boosting with:\n",
    "\n",
    "    n_estimators: 20\n",
    "    min_samples_split: 500\n",
    "    max_depth: 8\n",
    "    \n",
    "Then, let's plot our feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm2 = GradientBoostingClassifier(learning_rate=0.05,n_estimators = 20, \n",
    "                                               min_samples_split=500, min_samples_leaf=50,\n",
    "                                               max_depth=8, max_features='sqrt',\n",
    "                                               subsample=0.8, random_state=10)\n",
    "\n",
    "gbm2.fit(df_train[predictors], df_train[target])\n",
    "modelCV(gbm2, df_train, predictors)\n",
    "#Plot feature importance\n",
    "plt.style.use('seaborn-pastel')\n",
    "plotFeatureImp(gbm2, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can note that the feature late_fee is still the most important, followed by several categories for the original 'disposition' feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Let's model using logistic regression. We're expecting quicker results, but not as much accuracy as we'd like. Furthermore, we're going to only transform the data once for scaling purposes (note that this should be done in a pipeline with each change in cv fold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = LogisticRegression(random_state=10)\n",
    "\n",
    "# Scale the data from 0 to 1.0\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(df_train) # transform df_train values. Result is array\n",
    "df_train_scaled = pd.DataFrame(train_scaled, columns=df_train.columns) # put back into df with correct columns\n",
    "\n",
    "# Now fit the data\n",
    "lr0.fit(df_train_scaled[predictors], df_train_scaled[target])\n",
    "\n",
    "modelCV(lr0, df_train_scaled, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a logistic regression classifier with the default params, we can obtain a cv auc_score of 0.7855. This is lower than the first go-around with GBC (0.8023). Also, the accuracy of 0.9326 is slightly lower than GBC's to 0.9440. Unlike GBC, we can adjust the regularization parameter and see what we can produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'C': np.arange(30,201,10).tolist()}\n",
    "lrsearch1 = GridSearchCV(LogisticRegression(random_state=10), \n",
    "                        param_grid = param_test1, scoring='roc_auc',\n",
    "                        iid=False, cv=5)\n",
    "lrsearch1.fit(df_train_scaled[predictors],df_train_scaled[target])\n",
    "lrsearch1.best_params_, lrsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a C value of 40 optimizes the model a little more, with an auc score of 0.7872. For most cases, optimizing C (or trying different penalty scenarios with L1 and L2 against C) is enough for an LR classifier. Note that even after optimizing for regularization, the auc score is still below GBC. We will not continue to optimize this classifier as there was little gain with change in C vs the auc score being produced with GBC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
