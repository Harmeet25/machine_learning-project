{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdBntg3UTJg3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "subission_pd = pd.read_csv('submission6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "c8wedvYeT4nm",
    "outputId": "1a91f831-6556-49e3-dcba-e87c7567241e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (20972, 9)\n",
      "Test shape: (8989, 3)\n",
      "Sample shape: (8989, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape:',train.shape)\n",
    "print('Test shape:',test.shape)\n",
    "print('Sample shape:',subission_pd.shape)\n",
    "\n",
    "l = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_8-A4zRtT8Ss",
    "outputId": "91bb890b-601a-42ef-cb0c-1e2aaafcba28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18874, 2) (2098, 2)\n",
      "(18874, 6) (2098, 6)\n"
     ]
    }
   ],
   "source": [
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "X = train.loc[:,['TITLE','ABSTRACT']]\n",
    "y = train.loc[:,l]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "y1 = np.array(y_train)\n",
    "y2 = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENRy0yNfUaCK"
   },
   "outputs": [],
   "source": [
    "#Removing Punctuations\n",
    "\n",
    "X_train.replace('[^a-zA-Z]',' ', regex=True, inplace=True)\n",
    "X_test.replace('[^a-zA-Z]',' ', regex=True, inplace=True)\n",
    "\n",
    "test.replace('[^a-zA-Z]',' ', regex=True, inplace=True)\n",
    "\n",
    "#Converting to lower case characters\n",
    "\n",
    "for index in X_train.columns:\n",
    "  X_train[index] = X_train[index].str.lower()\n",
    "\n",
    "for index in X_test.columns:\n",
    "  X_test[index] = X_test[index].str.lower()\n",
    "\n",
    "for index in test.columns:\n",
    "  test[index] = test[index].str.lower()\n",
    "\n",
    "#Removing one letter words\n",
    "\n",
    "X_train['ABSTRACT'] = X_train['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "X_test['ABSTRACT'] = X_test['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "\n",
    "test['ABSTRACT'] = test['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "\n",
    "#Removing multiple blank spaces\n",
    "\n",
    "X_train = X_train.replace('\\s+', ' ', regex=True)\n",
    "X_test = X_test.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "test = test.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "VJD5hEUkUfNY",
    "outputId": "1b4eff7d-4a42-44a3-8e4e-ae4f2ef4a533"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:997)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/harmeetpurohit/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/harmeetpurohit/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/harmeetpurohit/Documents/ML project/Research topic Prediction/Research-topic-Prediction.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwordnet\u001b[39;00m \u001b[39mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# len(stop_words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# X_train['ABSTRACT'] = X_train['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# X_test['ABSTRACT'] = X_test['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# test['ABSTRACT'] = test['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harmeetpurohit/Documents/ML%20project/Research%20topic%20Prediction/Research-topic-Prediction.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m X_train[\u001b[39m'\u001b[39m\u001b[39mcombined\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m X_train[\u001b[39m'\u001b[39m\u001b[39mTITLE\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mX_train[\u001b[39m'\u001b[39m\u001b[39mABSTRACT\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/harmeetpurohit/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# len(stop_words)\n",
    "# X_train['ABSTRACT'] = X_train['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "# X_test['ABSTRACT'] = X_test['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "# test['ABSTRACT'] = test['ABSTRACT'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "X_train['combined'] = X_train['TITLE']+' '+X_train['ABSTRACT']\n",
    "X_test['combined'] = X_test['TITLE']+' '+X_test['ABSTRACT']\n",
    "\n",
    "test['combined'] = test['TITLE']+' '+test['ABSTRACT']\n",
    "\n",
    "X_train = X_train.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "X_test = X_test.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "\n",
    "test = test.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZpJCI5cYCap"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_lines = []\n",
    "for row in range(0,X.shape[0]):\n",
    "  X_lines.append(' '.join(str(x) for x in X.iloc[row,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKsTUdeRUrMd"
   },
   "outputs": [],
   "source": [
    "train_lines = []\n",
    "for row in range(0,X_train.shape[0]):\n",
    "  train_lines.append(' '.join(str(x) for x in X_train.iloc[row,:]))\n",
    "\n",
    "test_lines = []\n",
    "for row in range(0,X_test.shape[0]):\n",
    "  test_lines.append(' '.join(str(x) for x in X_test.iloc[row,:]))\n",
    "\n",
    "predtest_lines = []\n",
    "for row in range(0,test.shape[0]):\n",
    "  predtest_lines.append(' '.join(str(x) for x in test.iloc[row,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGDABzRWZlOi",
    "outputId": "a9321f83-b977-43e7-9d7b-243bfeeaa754"
   },
   "outputs": [],
   "source": [
    "len(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O05zD3CTUu_x"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvector = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_cv = countvector.fit_transform(train_lines)\n",
    "X_test_cv = countvector.transform(test_lines)\n",
    "\n",
    "test_cv = countvector.transform(predtest_lines)\n",
    "\n",
    "#Using TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "tfidfvector = TfidfTransformer()\n",
    "X_train_tf = tfidfvector.fit_transform(X_train_cv)\n",
    "X_test_tf = tfidfvector.fit_transform(X_test_cv)\n",
    "\n",
    "test_tf = tfidfvector.fit_transform(test_cv)\n",
    "\n",
    "X_cv = countvector.transform(X_lines)\n",
    "\n",
    "X_tf = tfidfvector.fit_transform(X_cv) #x_tf,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "CcnLYLSQV3wk",
    "outputId": "eec93633-b54c-4efb-9403-19467293f4c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "model = LinearSVC(C=0.5, class_weight='balanced', random_state=42)\n",
    "models = MultiOutputClassifier(model)\n",
    "\n",
    "models.fit(X_train_tf, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "MITfjxaeWGNf",
    "outputId": "b5b6e7e6-ac08-43ba-ced2-d19bd292fe39"
   },
   "outputs": [],
   "source": [
    "preds = models.predict(X_test_tf)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "f09g5ZevWI6e",
    "outputId": "a1410f03-caed-44f3-dd73-b19e1592f054"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#print(confusion_matrix(y2,preds))\n",
    "print(classification_report(y2,preds))\n",
    "print(accuracy_score(y2,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlszaq_ujFje"
   },
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(str(y2[i])+str(preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgQTqGrhjI7w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "mi9nVKeqWK-p",
    "outputId": "c27fb35b-0f4a-410e-ecdf-1b1b0aae4dae"
   },
   "outputs": [],
   "source": [
    "predssv = models.predict(test_tf)\n",
    "predssv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EBUnVCwlWPOZ",
    "outputId": "e99d8905-3d96-4471-8c2e-a43e2d4eef76"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/content/drive/My Drive/ml/test.csv')\n",
    "\n",
    "submit = pd.DataFrame({'ID': test.ID, 'Computer Science': predssv[:,0],'Physics':predssv[:,1],\n",
    "                       'Mathematics':predssv[:,2],'Statistics':predssv[:,3],'Quantitative Biology':predssv[:,4],\n",
    "                       'Quantitative Finance':predssv[:,5]})\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybS28j2AWbio"
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhmjaEvacjvO1SJCWFndxi",
   "collapsed_sections": [],
   "mount_file_id": "1-RAfjiGOy2brJkcnEiA6loSQDSn5oPQ1",
   "name": "independenceday_challange",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
